{
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30588,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/KuzmaKhrabrov/character-tokenizer.git"
      ],
      "metadata": {
        "id": "5HmwpqzwnMyE",
        "outputId": "bf7e212b-7a6f-43d5-ab64-fed8a37d3e55",
        "execution": {
          "iopub.status.busy": "2023-11-19T13:40:00.784265Z",
          "iopub.execute_input": "2023-11-19T13:40:00.784625Z",
          "iopub.status.idle": "2023-11-19T13:40:02.382053Z",
          "shell.execute_reply.started": "2023-11-19T13:40:00.784594Z",
          "shell.execute_reply": "2023-11-19T13:40:02.380926Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Cloning into 'character-tokenizer'...\nremote: Enumerating objects: 20, done.\u001b[K\nremote: Counting objects: 100% (20/20), done.\u001b[K\nremote: Compressing objects: 100% (14/14), done.\u001b[K\nremote: Total 20 (delta 5), reused 10 (delta 3), pack-reused 0\u001b[K\nReceiving objects: 100% (20/20), 5.89 KiB | 1.96 MiB/s, done.\nResolving deltas: 100% (5/5), done.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "hkVexA2nnRQ5",
        "execution": {
          "iopub.status.busy": "2023-11-19T13:19:07.727122Z",
          "iopub.execute_input": "2023-11-19T13:19:07.727431Z",
          "iopub.status.idle": "2023-11-19T13:19:20.365792Z",
          "shell.execute_reply.started": "2023-11-19T13:19:07.727404Z",
          "shell.execute_reply": "2023-11-19T13:19:20.364569Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/character-tokenizer/\")\n",
        "# sys.path.append(\"/kaggle/working/character-tokenizer\")"
      ],
      "metadata": {
        "id": "OW0UvWy7lNtp",
        "execution": {
          "iopub.status.busy": "2023-11-19T13:40:44.019530Z",
          "iopub.execute_input": "2023-11-19T13:40:44.020401Z",
          "iopub.status.idle": "2023-11-19T13:40:44.024777Z",
          "shell.execute_reply.started": "2023-11-19T13:40:44.020362Z",
          "shell.execute_reply": "2023-11-19T13:40:44.023923Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from charactertokenizer import CharacterTokenizer"
      ],
      "metadata": {
        "id": "5HMKP5W-7WPD",
        "execution": {
          "iopub.status.busy": "2023-11-19T13:40:45.780274Z",
          "iopub.execute_input": "2023-11-19T13:40:45.780624Z",
          "iopub.status.idle": "2023-11-19T13:40:50.675510Z",
          "shell.execute_reply.started": "2023-11-19T13:40:45.780593Z",
          "shell.execute_reply": "2023-11-19T13:40:50.674729Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание: обучите модель классификации букв для задачи расстановки ударения с помощью методов из библиотеки transformers. Датасет для обучения можно взять отсюда: https://github.com/Koziev/NLP_Datasets/blob/master/Stress/all_accents.zip\n",
        "\n",
        "1. Напишите класс для Dataset/Dataloder и разбейте данные на случайные train / test сплиты в соотношении 50:50. (1 балл)\n",
        "2. Попробуйте обучить одну или несколько из моделей: Bert, Albert, Deberta. Посчитайте метрику Accuracy на train и test. (1 балл). При преодолении порога в Accuracy на test 0.8: (+1 балл), 0.85: (+2 балла), 0.89: (+3 балла).\n",
        "Пример конфигурации для deberta: https://huggingface.co/IlyaGusev/ru-word-stress-transformer/blob/main/config.json"
      ],
      "metadata": {
        "id": "KQkp36rEoScR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('all_accents.tsv'):\n",
        "  url = \"https://github.com/Koziev/NLP_Datasets/raw/master/Stress/all_accents.zip\"\n",
        "  !wget $url -q\n",
        "  !unzip all_accents.zip\n",
        "  !rm all_accents.zip"
      ],
      "metadata": {
        "id": "mRVK6TNAZQFk",
        "execution": {
          "iopub.status.busy": "2023-11-19T13:40:50.677308Z",
          "iopub.execute_input": "2023-11-19T13:40:50.677825Z",
          "iopub.status.idle": "2023-11-19T13:40:54.402185Z",
          "shell.execute_reply.started": "2023-11-19T13:40:50.677789Z",
          "shell.execute_reply": "2023-11-19T13:40:54.400931Z"
        },
        "trusted": true,
        "outputId": "787c1be8-27d1-4776-cd0d-e1bda2bc458d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Archive:  all_accents.zip\n  inflating: all_accents.tsv         \n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "upl_CVce86ca",
        "outputId": "c046bb73-d344-4347-96fb-f0d456102d66",
        "execution": {
          "iopub.status.busy": "2023-11-19T13:40:54.403615Z",
          "iopub.execute_input": "2023-11-19T13:40:54.403949Z",
          "iopub.status.idle": "2023-11-19T13:40:54.453620Z",
          "shell.execute_reply.started": "2023-11-19T13:40:54.403919Z",
          "shell.execute_reply": "2023-11-19T13:40:54.452643Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "There are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = \"АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯя\"\n",
        "model_max_length = 64\n",
        "tokenizer = CharacterTokenizer(chars, model_max_length)"
      ],
      "metadata": {
        "id": "Sz1dO7KBCF93",
        "execution": {
          "iopub.status.busy": "2023-11-19T13:40:54.455815Z",
          "iopub.execute_input": "2023-11-19T13:40:54.456601Z",
          "iopub.status.idle": "2023-11-19T13:40:54.463958Z",
          "shell.execute_reply.started": "2023-11-19T13:40:54.456561Z",
          "shell.execute_reply": "2023-11-19T13:40:54.463029Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StressDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, path, tokenizer):\n",
        "        self.df = pd.read_csv(path, delimiter='\\t')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = tokenizer.model_max_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word, stress = self.df.iloc[idx]\n",
        "        encoded_dict = self.tokenizer.encode_plus(\n",
        "                    word,                          # Sentence to encode.\n",
        "                    add_special_tokens = True,     # Add '[CLS]' and '[SEP]'\n",
        "                    max_length = self.max_length, # Pad & truncate all sentences.\n",
        "                    padding='max_length',\n",
        "                    return_attention_mask = True,  # Construct attn. masks.\n",
        "                    return_tensors = 'pt',         # Return pytorch tensors.\n",
        "                   )\n",
        "        label = self.get_label(stress)\n",
        "        input_ids = encoded_dict['input_ids'].squeeze(0)\n",
        "        attention_mask = encoded_dict['attention_mask'].squeeze(0)\n",
        "        return input_ids, attention_mask, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def get_label(self, stress):\n",
        "        stress_count = 0\n",
        "        label = [-100] * self.max_length\n",
        "        word_len = len(stress) - stress.count('^')\n",
        "        label[1:word_len+1] = [0] * word_len\n",
        "\n",
        "        for i, c in enumerate(stress):\n",
        "            if c == '^':\n",
        "                label[i + 1 - stress_count] = 1\n",
        "                stress_count += 1\n",
        "        return torch.tensor(label)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-19T13:40:57.616993Z",
          "iopub.execute_input": "2023-11-19T13:40:57.617357Z",
          "iopub.status.idle": "2023-11-19T13:40:57.630731Z",
          "shell.execute_reply.started": "2023-11-19T13:40:57.617328Z",
          "shell.execute_reply": "2023-11-19T13:40:57.629718Z"
        },
        "trusted": true,
        "id": "poneewc8yySy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = StressDataset('all_accents.tsv', tokenizer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-19T13:41:00.252836Z",
          "iopub.execute_input": "2023-11-19T13:41:00.253211Z",
          "iopub.status.idle": "2023-11-19T13:41:03.210179Z",
          "shell.execute_reply.started": "2023-11-19T13:41:00.253180Z",
          "shell.execute_reply": "2023-11-19T13:41:03.209152Z"
        },
        "trusted": true,
        "id": "tUUvQsLHyySy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "\n",
        "train_size = int(0.5 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "metadata": {
        "id": "HuSrgQcdb2z6",
        "outputId": "d8a19bd0-3bd2-4131-c474-0b9c874ae6c9",
        "execution": {
          "iopub.status.busy": "2023-11-19T13:41:04.731461Z",
          "iopub.execute_input": "2023-11-19T13:41:04.731867Z",
          "iopub.status.idle": "2023-11-19T13:41:04.918524Z",
          "shell.execute_reply.started": "2023-11-19T13:41:04.731832Z",
          "shell.execute_reply": "2023-11-19T13:41:04.917498Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "840,267 training samples\n840,267 validation samples\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it\n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch\n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order.\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size, # Trains with this batch size.\n",
        "            drop_last=True\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size, # Evaluate with this batch size.\n",
        "        )"
      ],
      "metadata": {
        "id": "gkf6t47vdFSi",
        "execution": {
          "iopub.status.busy": "2023-11-19T13:41:22.411415Z",
          "iopub.execute_input": "2023-11-19T13:41:22.411794Z",
          "iopub.status.idle": "2023-11-19T13:41:22.418612Z",
          "shell.execute_reply.started": "2023-11-19T13:41:22.411762Z",
          "shell.execute_reply": "2023-11-19T13:41:22.417454Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DebertaV2ForTokenClassification, DebertaConfig, AdamW\n",
        "\n",
        "\n",
        "config_dict = {\n",
        "    \"architectures\": [\"DebertaV2ForTokenClassification\"],\n",
        "    \"attention_probs_dropout_prob\": 0.1,\n",
        "    \"hidden_act\": \"gelu\",\n",
        "    \"hidden_dropout_prob\": 0.1,\n",
        "    \"hidden_size\": 1024,\n",
        "    \"id2label\": {\"0\": \"NO\", \"1\": \"STRESS\"},\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"intermediate_size\": 4096,\n",
        "    \"label2id\": {\"NO\": 0, \"STRESS\": 1},\n",
        "    \"layer_norm_eps\": 1e-07,\n",
        "    \"max_length\": 64,\n",
        "    \"max_position_embeddings\": 66,\n",
        "    \"max_relative_positions\": -1,\n",
        "    \"model_type\": \"deberta-v2\",\n",
        "    \"num_attention_heads\": 8,\n",
        "    \"num_hidden_layers\": 4,\n",
        "    \"pad_token_id\": 4,\n",
        "    \"pooler_dropout\": 0,\n",
        "    \"pooler_hidden_act\": \"gelu\",\n",
        "    \"pooler_hidden_size\": 1024,\n",
        "    \"pos_att_type\": None,\n",
        "    \"position_biased_input\": True,\n",
        "    \"relative_attention\": False,\n",
        "    \"type_vocab_size\": 0,\n",
        "    \"vocab_size\": tokenizer.vocab_size\n",
        "}\n",
        "\n",
        "config = DebertaConfig.from_dict(config_dict)\n",
        "\n",
        "# Создание модели на основе конфигурации\n",
        "model = DebertaV2ForTokenClassification(config)\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "raK0lY-1h6cn",
        "execution": {
          "iopub.status.busy": "2023-11-19T13:41:46.240976Z",
          "iopub.execute_input": "2023-11-19T13:41:46.241365Z",
          "iopub.status.idle": "2023-11-19T13:41:50.719052Z",
          "shell.execute_reply.started": "2023-11-19T13:41:46.241332Z",
          "shell.execute_reply": "2023-11-19T13:41:50.718021Z"
        },
        "trusted": true,
        "outputId": "8116a007-4b4c-42f3-b1e1-8a4fb5a58f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DebertaV2ForTokenClassification(\n  (deberta): DebertaV2Model(\n    (embeddings): DebertaV2Embeddings(\n      (word_embeddings): Embedding(73, 1024, padding_idx=4)\n      (position_embeddings): Embedding(66, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n      (dropout): StableDropout()\n    )\n    (encoder): DebertaV2Encoder(\n      (layer): ModuleList(\n        (0-3): 4 x DebertaV2Layer(\n          (attention): DebertaV2Attention(\n            (self): DisentangledSelfAttention(\n              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): StableDropout()\n            )\n            (output): DebertaV2SelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n              (dropout): StableDropout()\n            )\n          )\n          (intermediate): DebertaV2Intermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaV2Output(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n            (dropout): StableDropout()\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-19T13:41:56.126801Z",
          "iopub.execute_input": "2023-11-19T13:41:56.127697Z",
          "iopub.status.idle": "2023-11-19T13:41:56.136179Z",
          "shell.execute_reply.started": "2023-11-19T13:41:56.127645Z",
          "shell.execute_reply": "2023-11-19T13:41:56.135125Z"
        },
        "trusted": true,
        "id": "wT9ir0PwyyS0",
        "outputId": "2245fe77-ef63-4569-fb9e-fc8758b37095"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "The BERT model has 70 different named parameters.\n\n==== Embedding Layer ====\n\ndeberta.embeddings.word_embeddings.weight                 (73, 1024)\ndeberta.embeddings.position_embeddings.weight             (66, 1024)\ndeberta.embeddings.LayerNorm.weight                          (1024,)\ndeberta.embeddings.LayerNorm.bias                            (1024,)\ndeberta.encoder.layer.0.attention.self.query_proj.weight (1024, 1024)\n\n==== First Transformer ====\n\ndeberta.encoder.layer.0.attention.self.query_proj.bias       (1024,)\ndeberta.encoder.layer.0.attention.self.key_proj.weight  (1024, 1024)\ndeberta.encoder.layer.0.attention.self.key_proj.bias         (1024,)\ndeberta.encoder.layer.0.attention.self.value_proj.weight (1024, 1024)\ndeberta.encoder.layer.0.attention.self.value_proj.bias       (1024,)\ndeberta.encoder.layer.0.attention.output.dense.weight   (1024, 1024)\ndeberta.encoder.layer.0.attention.output.dense.bias          (1024,)\ndeberta.encoder.layer.0.attention.output.LayerNorm.weight      (1024,)\ndeberta.encoder.layer.0.attention.output.LayerNorm.bias      (1024,)\ndeberta.encoder.layer.0.intermediate.dense.weight       (4096, 1024)\ndeberta.encoder.layer.0.intermediate.dense.bias              (4096,)\ndeberta.encoder.layer.0.output.dense.weight             (1024, 4096)\ndeberta.encoder.layer.0.output.dense.bias                    (1024,)\ndeberta.encoder.layer.0.output.LayerNorm.weight              (1024,)\ndeberta.encoder.layer.0.output.LayerNorm.bias                (1024,)\ndeberta.encoder.layer.1.attention.self.query_proj.weight (1024, 1024)\n\n==== Output Layer ====\n\ndeberta.encoder.layer.3.output.LayerNorm.weight              (1024,)\ndeberta.encoder.layer.3.output.LayerNorm.bias                (1024,)\nclassifier.weight                                          (2, 1024)\nclassifier.bias                                                 (2,)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-5, eps = 1e-8)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-19T13:41:59.246361Z",
          "iopub.execute_input": "2023-11-19T13:41:59.247060Z",
          "iopub.status.idle": "2023-11-19T13:41:59.251909Z",
          "shell.execute_reply.started": "2023-11-19T13:41:59.247027Z",
          "shell.execute_reply": "2023-11-19T13:41:59.251024Z"
        },
        "trusted": true,
        "id": "m_dgA9V1yyS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4.\n",
        "epochs = 2\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-19T13:42:03.366552Z",
          "iopub.execute_input": "2023-11-19T13:42:03.367441Z",
          "iopub.status.idle": "2023-11-19T13:42:03.372541Z",
          "shell.execute_reply.started": "2023-11-19T13:42:03.367405Z",
          "shell.execute_reply": "2023-11-19T13:42:03.371642Z"
        },
        "trusted": true,
        "id": "aKgKHUIWyyS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "iRQSc7KxptYy",
        "execution": {
          "iopub.status.busy": "2023-11-19T13:42:08.504595Z",
          "iopub.execute_input": "2023-11-19T13:42:08.505476Z",
          "iopub.status.idle": "2023-11-19T13:42:08.510593Z",
          "shell.execute_reply.started": "2023-11-19T13:42:08.505438Z",
          "shell.execute_reply": "2023-11-19T13:42:08.509505Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def accuracy(preds, labels):\n",
        "    preds = np.argmax(preds, axis=2)\n",
        "    true_preds = [\n",
        "        [p for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(preds, labels)\n",
        "    ]\n",
        "    true_labels= [\n",
        "        [l for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(preds, labels)\n",
        "    ]\n",
        "    s = 0\n",
        "    for pred, label in zip(true_preds, true_labels):\n",
        "        if pred == label:\n",
        "            s += 1\n",
        "    return s / len(labels)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-19T13:42:09.951270Z",
          "iopub.execute_input": "2023-11-19T13:42:09.951895Z",
          "iopub.status.idle": "2023-11-19T13:42:09.958692Z",
          "shell.execute_reply.started": "2023-11-19T13:42:09.951864Z",
          "shell.execute_reply": "2023-11-19T13:42:09.957728Z"
        },
        "trusted": true,
        "id": "LF748E6hyyS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss,\n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 1000 batches.\n",
        "        if step % 100 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "            print(f'  Average Loss: {total_train_loss / step}')\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        output = model(b_input_ids,\n",
        "                             token_type_ids=None,\n",
        "                             attention_mask=b_input_mask,\n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_train_loss += output.loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        output.loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            output = model(b_input_ids,\n",
        "                                   token_type_ids=None,\n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += output.loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += accuracy(logits, label_ids)\n",
        "\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "metadata": {
        "id": "WMQ5_p4wpyyo",
        "outputId": "68ba840a-c0b1-406c-bbf7-2dd5373ba15c",
        "execution": {
          "iopub.status.busy": "2023-11-19T13:42:15.531525Z",
          "iopub.execute_input": "2023-11-19T13:42:15.531910Z",
          "iopub.status.idle": "2023-11-19T16:04:52.685949Z",
          "shell.execute_reply.started": "2023-11-19T13:42:15.531876Z",
          "shell.execute_reply": "2023-11-19T16:04:52.684862Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n======== Epoch 1 / 2 ========\nTraining...\n  Batch   100  of  26,258.    Elapsed: 0:00:14.\n  Average Loss: 0.2224211063981056\n  Batch   200  of  26,258.    Elapsed: 0:00:26.\n  Average Loss: 0.19335158694535493\n  Batch   300  of  26,258.    Elapsed: 0:00:37.\n  Average Loss: 0.1790248303115368\n  Batch   400  of  26,258.    Elapsed: 0:00:49.\n  Average Loss: 0.17005230886861683\n  Batch   500  of  26,258.    Elapsed: 0:01:01.\n  Average Loss: 0.16388145512342453\n  Batch   600  of  26,258.    Elapsed: 0:01:12.\n  Average Loss: 0.1589123268549641\n  Batch   700  of  26,258.    Elapsed: 0:01:24.\n  Average Loss: 0.15454400038080557\n  Batch   800  of  26,258.    Elapsed: 0:01:36.\n  Average Loss: 0.15087683737277985\n  Batch   900  of  26,258.    Elapsed: 0:01:47.\n  Average Loss: 0.1478652508225706\n  Batch 1,000  of  26,258.    Elapsed: 0:01:59.\n  Average Loss: 0.1453603562042117\n  Batch 1,100  of  26,258.    Elapsed: 0:02:11.\n  Average Loss: 0.1432896817949685\n  Batch 1,200  of  26,258.    Elapsed: 0:02:23.\n  Average Loss: 0.14116148308540383\n  Batch 1,300  of  26,258.    Elapsed: 0:02:34.\n  Average Loss: 0.13919647056895953\n  Batch 1,400  of  26,258.    Elapsed: 0:02:46.\n  Average Loss: 0.13737188187294772\n  Batch 1,500  of  26,258.    Elapsed: 0:02:58.\n  Average Loss: 0.1356451010927558\n  Batch 1,600  of  26,258.    Elapsed: 0:03:09.\n  Average Loss: 0.13450076402863487\n  Batch 1,700  of  26,258.    Elapsed: 0:03:21.\n  Average Loss: 0.13319033983875722\n  Batch 1,800  of  26,258.    Elapsed: 0:03:33.\n  Average Loss: 0.13198605231319865\n  Batch 1,900  of  26,258.    Elapsed: 0:03:44.\n  Average Loss: 0.13070166300982236\n  Batch 2,000  of  26,258.    Elapsed: 0:03:56.\n  Average Loss: 0.12972357032634318\n  Batch 2,100  of  26,258.    Elapsed: 0:04:08.\n  Average Loss: 0.12859306775033474\n  Batch 2,200  of  26,258.    Elapsed: 0:04:19.\n  Average Loss: 0.12775576785206794\n  Batch 2,300  of  26,258.    Elapsed: 0:04:31.\n  Average Loss: 0.12687223777660858\n  Batch 2,400  of  26,258.    Elapsed: 0:04:43.\n  Average Loss: 0.1261618818420296\n  Batch 2,500  of  26,258.    Elapsed: 0:04:55.\n  Average Loss: 0.12534271603226663\n  Batch 2,600  of  26,258.    Elapsed: 0:05:06.\n  Average Loss: 0.12455234132420558\n  Batch 2,700  of  26,258.    Elapsed: 0:05:18.\n  Average Loss: 0.1238039801173188\n  Batch 2,800  of  26,258.    Elapsed: 0:05:30.\n  Average Loss: 0.12303759977487581\n  Batch 2,900  of  26,258.    Elapsed: 0:05:41.\n  Average Loss: 0.12252761321848836\n  Batch 3,000  of  26,258.    Elapsed: 0:05:53.\n  Average Loss: 0.12183299841235082\n  Batch 3,100  of  26,258.    Elapsed: 0:06:05.\n  Average Loss: 0.12124061334277353\n  Batch 3,200  of  26,258.    Elapsed: 0:06:16.\n  Average Loss: 0.12063731570378877\n  Batch 3,300  of  26,258.    Elapsed: 0:06:28.\n  Average Loss: 0.12011375420924389\n  Batch 3,400  of  26,258.    Elapsed: 0:06:40.\n  Average Loss: 0.11956102044924218\n  Batch 3,500  of  26,258.    Elapsed: 0:06:52.\n  Average Loss: 0.11887359191796609\n  Batch 3,600  of  26,258.    Elapsed: 0:07:03.\n  Average Loss: 0.11838063661319514\n  Batch 3,700  of  26,258.    Elapsed: 0:07:15.\n  Average Loss: 0.11787719812546227\n  Batch 3,800  of  26,258.    Elapsed: 0:07:27.\n  Average Loss: 0.11744719099057348\n  Batch 3,900  of  26,258.    Elapsed: 0:07:38.\n  Average Loss: 0.1170105208762181\n  Batch 4,000  of  26,258.    Elapsed: 0:07:50.\n  Average Loss: 0.1165288452077657\n  Batch 4,100  of  26,258.    Elapsed: 0:08:02.\n  Average Loss: 0.11613104563751599\n  Batch 4,200  of  26,258.    Elapsed: 0:08:13.\n  Average Loss: 0.11576641269205581\n  Batch 4,300  of  26,258.    Elapsed: 0:08:25.\n  Average Loss: 0.11537058011390441\n  Batch 4,400  of  26,258.    Elapsed: 0:08:37.\n  Average Loss: 0.11491660864600403\n  Batch 4,500  of  26,258.    Elapsed: 0:08:49.\n  Average Loss: 0.11460540505829785\n  Batch 4,600  of  26,258.    Elapsed: 0:09:00.\n  Average Loss: 0.11423977454881305\n  Batch 4,700  of  26,258.    Elapsed: 0:09:12.\n  Average Loss: 0.11390081182835585\n  Batch 4,800  of  26,258.    Elapsed: 0:09:24.\n  Average Loss: 0.11345164292802414\n  Batch 4,900  of  26,258.    Elapsed: 0:09:35.\n  Average Loss: 0.1130479107150922\n  Batch 5,000  of  26,258.    Elapsed: 0:09:47.\n  Average Loss: 0.11266753058508039\n  Batch 5,100  of  26,258.    Elapsed: 0:09:59.\n  Average Loss: 0.1122951779346548\n  Batch 5,200  of  26,258.    Elapsed: 0:10:10.\n  Average Loss: 0.1119509497585778\n  Batch 5,300  of  26,258.    Elapsed: 0:10:22.\n  Average Loss: 0.11159061604864755\n  Batch 5,400  of  26,258.    Elapsed: 0:10:34.\n  Average Loss: 0.11128674350609934\n  Batch 5,500  of  26,258.    Elapsed: 0:10:46.\n  Average Loss: 0.11096901430054144\n  Batch 5,600  of  26,258.    Elapsed: 0:10:57.\n  Average Loss: 0.11067850752201464\n  Batch 5,700  of  26,258.    Elapsed: 0:11:09.\n  Average Loss: 0.11034145203598758\n  Batch 5,800  of  26,258.    Elapsed: 0:11:21.\n  Average Loss: 0.11010929624453701\n  Batch 5,900  of  26,258.    Elapsed: 0:11:32.\n  Average Loss: 0.1098713081342689\n  Batch 6,000  of  26,258.    Elapsed: 0:11:44.\n  Average Loss: 0.10959002426080405\n  Batch 6,100  of  26,258.    Elapsed: 0:11:56.\n  Average Loss: 0.10933248883388082\n  Batch 6,200  of  26,258.    Elapsed: 0:12:07.\n  Average Loss: 0.10905507931365602\n  Batch 6,300  of  26,258.    Elapsed: 0:12:19.\n  Average Loss: 0.10880899556513343\n  Batch 6,400  of  26,258.    Elapsed: 0:12:31.\n  Average Loss: 0.1085830051539233\n  Batch 6,500  of  26,258.    Elapsed: 0:12:43.\n  Average Loss: 0.10830446497981365\n  Batch 6,600  of  26,258.    Elapsed: 0:12:54.\n  Average Loss: 0.10808264072645794\n  Batch 6,700  of  26,258.    Elapsed: 0:13:06.\n  Average Loss: 0.10785013533795058\n  Batch 6,800  of  26,258.    Elapsed: 0:13:18.\n  Average Loss: 0.10752310139839263\n  Batch 6,900  of  26,258.    Elapsed: 0:13:29.\n  Average Loss: 0.10731056874644929\n  Batch 7,000  of  26,258.    Elapsed: 0:13:41.\n  Average Loss: 0.1070820365782295\n  Batch 7,100  of  26,258.    Elapsed: 0:13:53.\n  Average Loss: 0.10681096566080207\n  Batch 7,200  of  26,258.    Elapsed: 0:14:04.\n  Average Loss: 0.10653498994600441\n  Batch 7,300  of  26,258.    Elapsed: 0:14:16.\n  Average Loss: 0.1063614307201072\n  Batch 7,400  of  26,258.    Elapsed: 0:14:28.\n  Average Loss: 0.10613818004979073\n  Batch 7,500  of  26,258.    Elapsed: 0:14:39.\n  Average Loss: 0.10591188219487667\n  Batch 7,600  of  26,258.    Elapsed: 0:14:51.\n  Average Loss: 0.10564928340235431\n  Batch 7,700  of  26,258.    Elapsed: 0:15:03.\n  Average Loss: 0.10543248735319871\n  Batch 7,800  of  26,258.    Elapsed: 0:15:15.\n  Average Loss: 0.10523239775298115\n  Batch 7,900  of  26,258.    Elapsed: 0:15:26.\n  Average Loss: 0.10500220754337085\n  Batch 8,000  of  26,258.    Elapsed: 0:15:38.\n  Average Loss: 0.10482587128877639\n  Batch 8,100  of  26,258.    Elapsed: 0:15:50.\n  Average Loss: 0.10462641630505706\n  Batch 8,200  of  26,258.    Elapsed: 0:16:02.\n  Average Loss: 0.10441906068519485\n  Batch 8,300  of  26,258.    Elapsed: 0:16:13.\n  Average Loss: 0.10419860856540232\n  Batch 8,400  of  26,258.    Elapsed: 0:16:25.\n  Average Loss: 0.10399444840404959\n  Batch 8,500  of  26,258.    Elapsed: 0:16:37.\n  Average Loss: 0.10377813617797459\n  Batch 8,600  of  26,258.    Elapsed: 0:16:49.\n  Average Loss: 0.103576520743325\n  Batch 8,700  of  26,258.    Elapsed: 0:17:00.\n  Average Loss: 0.10338385926795074\n  Batch 8,800  of  26,258.    Elapsed: 0:17:12.\n  Average Loss: 0.10320032219614156\n  Batch 8,900  of  26,258.    Elapsed: 0:17:24.\n  Average Loss: 0.10302845253033585\n  Batch 9,000  of  26,258.    Elapsed: 0:17:35.\n  Average Loss: 0.1028108640284174\n  Batch 9,100  of  26,258.    Elapsed: 0:17:47.\n  Average Loss: 0.1026048241805408\n  Batch 9,200  of  26,258.    Elapsed: 0:17:59.\n  Average Loss: 0.10245795176485958\n  Batch 9,300  of  26,258.    Elapsed: 0:18:11.\n  Average Loss: 0.10229106381014791\n  Batch 9,400  of  26,258.    Elapsed: 0:18:22.\n  Average Loss: 0.10211883328260576\n  Batch 9,500  of  26,258.    Elapsed: 0:18:34.\n  Average Loss: 0.10193244691624453\n  Batch 9,600  of  26,258.    Elapsed: 0:18:46.\n  Average Loss: 0.10172570185425381\n  Batch 9,700  of  26,258.    Elapsed: 0:18:57.\n  Average Loss: 0.10157280474632364\n  Batch 9,800  of  26,258.    Elapsed: 0:19:09.\n  Average Loss: 0.1014190044307283\n  Batch 9,900  of  26,258.    Elapsed: 0:19:21.\n  Average Loss: 0.10124181559742099\n  Batch 10,000  of  26,258.    Elapsed: 0:19:32.\n  Average Loss: 0.10105724553018808\n  Batch 10,100  of  26,258.    Elapsed: 0:19:44.\n  Average Loss: 0.10087314439882146\n  Batch 10,200  of  26,258.    Elapsed: 0:19:56.\n  Average Loss: 0.10069994560424604\n  Batch 10,300  of  26,258.    Elapsed: 0:20:08.\n  Average Loss: 0.10055424662722835\n  Batch 10,400  of  26,258.    Elapsed: 0:20:19.\n  Average Loss: 0.10040318165726674\n  Batch 10,500  of  26,258.    Elapsed: 0:20:31.\n  Average Loss: 0.10026360365287179\n  Batch 10,600  of  26,258.    Elapsed: 0:20:43.\n  Average Loss: 0.10007800955957961\n  Batch 10,700  of  26,258.    Elapsed: 0:20:54.\n  Average Loss: 0.09990548613838085\n  Batch 10,800  of  26,258.    Elapsed: 0:21:06.\n  Average Loss: 0.09971465632026257\n  Batch 10,900  of  26,258.    Elapsed: 0:21:18.\n  Average Loss: 0.0995496020966782\n  Batch 11,000  of  26,258.    Elapsed: 0:21:29.\n  Average Loss: 0.099359996128658\n  Batch 11,100  of  26,258.    Elapsed: 0:21:41.\n  Average Loss: 0.09917593028413149\n  Batch 11,200  of  26,258.    Elapsed: 0:21:53.\n  Average Loss: 0.09903703203341657\n  Batch 11,300  of  26,258.    Elapsed: 0:22:05.\n  Average Loss: 0.09890029714615867\n  Batch 11,400  of  26,258.    Elapsed: 0:22:16.\n  Average Loss: 0.09874275463012358\n  Batch 11,500  of  26,258.    Elapsed: 0:22:28.\n  Average Loss: 0.09856051476186384\n  Batch 11,600  of  26,258.    Elapsed: 0:22:40.\n  Average Loss: 0.09843557306645631\n  Batch 11,700  of  26,258.    Elapsed: 0:22:51.\n  Average Loss: 0.09828983009323224\n  Batch 11,800  of  26,258.    Elapsed: 0:23:03.\n  Average Loss: 0.09815324250719179\n  Batch 11,900  of  26,258.    Elapsed: 0:23:15.\n  Average Loss: 0.09798640504463756\n  Batch 12,000  of  26,258.    Elapsed: 0:23:26.\n  Average Loss: 0.09784599062226092\n  Batch 12,100  of  26,258.    Elapsed: 0:23:38.\n  Average Loss: 0.09768266477968451\n  Batch 12,200  of  26,258.    Elapsed: 0:23:50.\n  Average Loss: 0.09755732157664587\n  Batch 12,300  of  26,258.    Elapsed: 0:24:02.\n  Average Loss: 0.09740952883566904\n  Batch 12,400  of  26,258.    Elapsed: 0:24:13.\n  Average Loss: 0.09723964207249904\n  Batch 12,500  of  26,258.    Elapsed: 0:24:25.\n  Average Loss: 0.09712311215266585\n  Batch 12,600  of  26,258.    Elapsed: 0:24:37.\n  Average Loss: 0.09693844784952936\n  Batch 12,700  of  26,258.    Elapsed: 0:24:48.\n  Average Loss: 0.09680544928317582\n  Batch 12,800  of  26,258.    Elapsed: 0:25:00.\n  Average Loss: 0.09668524788357899\n  Batch 12,900  of  26,258.    Elapsed: 0:25:12.\n  Average Loss: 0.09656389714139262\n  Batch 13,000  of  26,258.    Elapsed: 0:25:23.\n  Average Loss: 0.09639741234696256\n  Batch 13,100  of  26,258.    Elapsed: 0:25:35.\n  Average Loss: 0.09624372728140528\n  Batch 13,200  of  26,258.    Elapsed: 0:25:47.\n  Average Loss: 0.09607502602435875\n  Batch 13,300  of  26,258.    Elapsed: 0:25:59.\n  Average Loss: 0.09593690901280458\n  Batch 13,400  of  26,258.    Elapsed: 0:26:10.\n  Average Loss: 0.09582240200573718\n  Batch 13,500  of  26,258.    Elapsed: 0:26:22.\n  Average Loss: 0.09566027537222814\n  Batch 13,600  of  26,258.    Elapsed: 0:26:34.\n  Average Loss: 0.0955483632700463\n  Batch 13,700  of  26,258.    Elapsed: 0:26:45.\n  Average Loss: 0.09540523744045491\n  Batch 13,800  of  26,258.    Elapsed: 0:26:57.\n  Average Loss: 0.09527810377662704\n  Batch 13,900  of  26,258.    Elapsed: 0:27:09.\n  Average Loss: 0.09511349071326444\n  Batch 14,000  of  26,258.    Elapsed: 0:27:20.\n  Average Loss: 0.0949684017800859\n  Batch 14,100  of  26,258.    Elapsed: 0:27:32.\n  Average Loss: 0.09484012996421215\n  Batch 14,200  of  26,258.    Elapsed: 0:27:44.\n  Average Loss: 0.09467541844413524\n  Batch 14,300  of  26,258.    Elapsed: 0:27:55.\n  Average Loss: 0.09456743503770941\n  Batch 14,400  of  26,258.    Elapsed: 0:28:07.\n  Average Loss: 0.09443704360617428\n  Batch 14,500  of  26,258.    Elapsed: 0:28:19.\n  Average Loss: 0.09431018738358699\n  Batch 14,600  of  26,258.    Elapsed: 0:28:31.\n  Average Loss: 0.09417310109476827\n  Batch 14,700  of  26,258.    Elapsed: 0:28:42.\n  Average Loss: 0.09404911066513179\n  Batch 14,800  of  26,258.    Elapsed: 0:28:54.\n  Average Loss: 0.09392303077592137\n  Batch 14,900  of  26,258.    Elapsed: 0:29:06.\n  Average Loss: 0.09377418557156952\n  Batch 15,000  of  26,258.    Elapsed: 0:29:18.\n  Average Loss: 0.0936279650768886\n  Batch 15,100  of  26,258.    Elapsed: 0:29:29.\n  Average Loss: 0.0934860728896957\n  Batch 15,200  of  26,258.    Elapsed: 0:29:41.\n  Average Loss: 0.09337738260541013\n  Batch 15,300  of  26,258.    Elapsed: 0:29:53.\n  Average Loss: 0.09323581305533356\n  Batch 15,400  of  26,258.    Elapsed: 0:30:04.\n  Average Loss: 0.09309781780055204\n  Batch 15,500  of  26,258.    Elapsed: 0:30:16.\n  Average Loss: 0.09294166693031307\n  Batch 15,600  of  26,258.    Elapsed: 0:30:28.\n  Average Loss: 0.09281924681272358\n  Batch 15,700  of  26,258.    Elapsed: 0:30:39.\n  Average Loss: 0.09270228488536872\n  Batch 15,800  of  26,258.    Elapsed: 0:30:51.\n  Average Loss: 0.09255755813474048\n  Batch 15,900  of  26,258.    Elapsed: 0:31:03.\n  Average Loss: 0.09241437813161679\n  Batch 16,000  of  26,258.    Elapsed: 0:31:15.\n  Average Loss: 0.09228319259174168\n  Batch 16,100  of  26,258.    Elapsed: 0:31:26.\n  Average Loss: 0.09214417145037966\n  Batch 16,200  of  26,258.    Elapsed: 0:31:38.\n  Average Loss: 0.09203233250646771\n  Batch 16,300  of  26,258.    Elapsed: 0:31:50.\n  Average Loss: 0.09190462028619154\n  Batch 16,400  of  26,258.    Elapsed: 0:32:01.\n  Average Loss: 0.09179679355175212\n  Batch 16,500  of  26,258.    Elapsed: 0:32:13.\n  Average Loss: 0.09165888291315147\n  Batch 16,600  of  26,258.    Elapsed: 0:32:25.\n  Average Loss: 0.091541121378541\n  Batch 16,700  of  26,258.    Elapsed: 0:32:36.\n  Average Loss: 0.09140426769518031\n  Batch 16,800  of  26,258.    Elapsed: 0:32:48.\n  Average Loss: 0.09127306153742774\n  Batch 16,900  of  26,258.    Elapsed: 0:33:00.\n  Average Loss: 0.09116681757774314\n  Batch 17,000  of  26,258.    Elapsed: 0:33:11.\n  Average Loss: 0.0910211323162212\n  Batch 17,100  of  26,258.    Elapsed: 0:33:23.\n  Average Loss: 0.09091439432633376\n  Batch 17,200  of  26,258.    Elapsed: 0:33:35.\n  Average Loss: 0.09078403586673356\n  Batch 17,300  of  26,258.    Elapsed: 0:33:47.\n  Average Loss: 0.09066039567447208\n  Batch 17,400  of  26,258.    Elapsed: 0:33:58.\n  Average Loss: 0.09052762892867984\n  Batch 17,500  of  26,258.    Elapsed: 0:34:10.\n  Average Loss: 0.09041864615572351\n  Batch 17,600  of  26,258.    Elapsed: 0:34:22.\n  Average Loss: 0.09029480023584752\n  Batch 17,700  of  26,258.    Elapsed: 0:34:33.\n  Average Loss: 0.09020598090174844\n  Batch 17,800  of  26,258.    Elapsed: 0:34:45.\n  Average Loss: 0.09010277041307325\n  Batch 17,900  of  26,258.    Elapsed: 0:34:57.\n  Average Loss: 0.08998703395979031\n  Batch 18,000  of  26,258.    Elapsed: 0:35:08.\n  Average Loss: 0.08985742780204034\n  Batch 18,100  of  26,258.    Elapsed: 0:35:20.\n  Average Loss: 0.08973893861610505\n  Batch 18,200  of  26,258.    Elapsed: 0:35:32.\n  Average Loss: 0.08961643906566058\n  Batch 18,300  of  26,258.    Elapsed: 0:35:43.\n  Average Loss: 0.08950394919345694\n  Batch 18,400  of  26,258.    Elapsed: 0:35:55.\n  Average Loss: 0.08937990494943021\n  Batch 18,500  of  26,258.    Elapsed: 0:36:07.\n  Average Loss: 0.08927090055898235\n  Batch 18,600  of  26,258.    Elapsed: 0:36:19.\n  Average Loss: 0.08917558654822329\n  Batch 18,700  of  26,258.    Elapsed: 0:36:30.\n  Average Loss: 0.08906606531388221\n  Batch 18,800  of  26,258.    Elapsed: 0:36:42.\n  Average Loss: 0.0889609170657523\n  Batch 18,900  of  26,258.    Elapsed: 0:36:54.\n  Average Loss: 0.08885064206327553\n  Batch 19,000  of  26,258.    Elapsed: 0:37:05.\n  Average Loss: 0.08875440772779677\n  Batch 19,100  of  26,258.    Elapsed: 0:37:17.\n  Average Loss: 0.088635754834529\n  Batch 19,200  of  26,258.    Elapsed: 0:37:29.\n  Average Loss: 0.08853814924339531\n  Batch 19,300  of  26,258.    Elapsed: 0:37:40.\n  Average Loss: 0.08843705011087764\n  Batch 19,400  of  26,258.    Elapsed: 0:37:52.\n  Average Loss: 0.0883033461945573\n  Batch 19,500  of  26,258.    Elapsed: 0:38:04.\n  Average Loss: 0.0882006887327402\n  Batch 19,600  of  26,258.    Elapsed: 0:38:15.\n  Average Loss: 0.08809551997918028\n  Batch 19,700  of  26,258.    Elapsed: 0:38:27.\n  Average Loss: 0.08798557878580644\n  Batch 19,800  of  26,258.    Elapsed: 0:38:39.\n  Average Loss: 0.08787404306825589\n  Batch 19,900  of  26,258.    Elapsed: 0:38:50.\n  Average Loss: 0.08776054080996906\n  Batch 20,000  of  26,258.    Elapsed: 0:39:02.\n  Average Loss: 0.08764597902847454\n  Batch 20,100  of  26,258.    Elapsed: 0:39:14.\n  Average Loss: 0.08753084706791227\n  Batch 20,200  of  26,258.    Elapsed: 0:39:26.\n  Average Loss: 0.0874145711146959\n  Batch 20,300  of  26,258.    Elapsed: 0:39:37.\n  Average Loss: 0.08730511062966632\n  Batch 20,400  of  26,258.    Elapsed: 0:39:49.\n  Average Loss: 0.08718484190493529\n  Batch 20,500  of  26,258.    Elapsed: 0:40:01.\n  Average Loss: 0.08706645303528483\n  Batch 20,600  of  26,258.    Elapsed: 0:40:12.\n  Average Loss: 0.08693591200882225\n  Batch 20,700  of  26,258.    Elapsed: 0:40:24.\n  Average Loss: 0.08682245080291794\n  Batch 20,800  of  26,258.    Elapsed: 0:40:36.\n  Average Loss: 0.08672765248884948\n  Batch 20,900  of  26,258.    Elapsed: 0:40:47.\n  Average Loss: 0.08665977688889469\n  Batch 21,000  of  26,258.    Elapsed: 0:40:59.\n  Average Loss: 0.0865594485971101\n  Batch 21,100  of  26,258.    Elapsed: 0:41:11.\n  Average Loss: 0.0864603856382053\n  Batch 21,200  of  26,258.    Elapsed: 0:41:23.\n  Average Loss: 0.08636733163609434\n  Batch 21,300  of  26,258.    Elapsed: 0:41:34.\n  Average Loss: 0.0862560865007988\n  Batch 21,400  of  26,258.    Elapsed: 0:41:46.\n  Average Loss: 0.08615230656981851\n  Batch 21,500  of  26,258.    Elapsed: 0:41:58.\n  Average Loss: 0.08603895108094223\n  Batch 21,600  of  26,258.    Elapsed: 0:42:09.\n  Average Loss: 0.08592363145323126\n  Batch 21,700  of  26,258.    Elapsed: 0:42:21.\n  Average Loss: 0.08582858386453808\n  Batch 21,800  of  26,258.    Elapsed: 0:42:33.\n  Average Loss: 0.08572378769388782\n  Batch 21,900  of  26,258.    Elapsed: 0:42:45.\n  Average Loss: 0.08563000076412133\n  Batch 22,000  of  26,258.    Elapsed: 0:42:56.\n  Average Loss: 0.08552422470209951\n  Batch 22,100  of  26,258.    Elapsed: 0:43:08.\n  Average Loss: 0.08539890596790585\n  Batch 22,200  of  26,258.    Elapsed: 0:43:20.\n  Average Loss: 0.0852705414614615\n  Batch 22,300  of  26,258.    Elapsed: 0:43:31.\n  Average Loss: 0.08516062808765992\n  Batch 22,400  of  26,258.    Elapsed: 0:43:43.\n  Average Loss: 0.08505168192990822\n  Batch 22,500  of  26,258.    Elapsed: 0:43:55.\n  Average Loss: 0.08494843974837826\n  Batch 22,600  of  26,258.    Elapsed: 0:44:06.\n  Average Loss: 0.08485723062785161\n  Batch 22,700  of  26,258.    Elapsed: 0:44:18.\n  Average Loss: 0.08475032321989766\n  Batch 22,800  of  26,258.    Elapsed: 0:44:30.\n  Average Loss: 0.08464668954162972\n  Batch 22,900  of  26,258.    Elapsed: 0:44:41.\n  Average Loss: 0.08455248037064407\n  Batch 23,000  of  26,258.    Elapsed: 0:44:53.\n  Average Loss: 0.08446531555016079\n  Batch 23,100  of  26,258.    Elapsed: 0:45:05.\n  Average Loss: 0.08436002817303265\n  Batch 23,200  of  26,258.    Elapsed: 0:45:17.\n  Average Loss: 0.08425020406311699\n  Batch 23,300  of  26,258.    Elapsed: 0:45:28.\n  Average Loss: 0.08414799089547519\n  Batch 23,400  of  26,258.    Elapsed: 0:45:40.\n  Average Loss: 0.08404941383040805\n  Batch 23,500  of  26,258.    Elapsed: 0:45:52.\n  Average Loss: 0.08392412414203616\n  Batch 23,600  of  26,258.    Elapsed: 0:46:03.\n  Average Loss: 0.08383636840208733\n  Batch 23,700  of  26,258.    Elapsed: 0:46:15.\n  Average Loss: 0.08374334742271887\n  Batch 23,800  of  26,258.    Elapsed: 0:46:27.\n  Average Loss: 0.08364393284340867\n  Batch 23,900  of  26,258.    Elapsed: 0:46:39.\n  Average Loss: 0.08355104934260499\n  Batch 24,000  of  26,258.    Elapsed: 0:46:50.\n  Average Loss: 0.08345982697916528\n  Batch 24,100  of  26,258.    Elapsed: 0:47:02.\n  Average Loss: 0.08336998114752664\n  Batch 24,200  of  26,258.    Elapsed: 0:47:14.\n  Average Loss: 0.08327384639273529\n  Batch 24,300  of  26,258.    Elapsed: 0:47:26.\n  Average Loss: 0.0831685364036725\n  Batch 24,400  of  26,258.    Elapsed: 0:47:37.\n  Average Loss: 0.08307245083355544\n  Batch 24,500  of  26,258.    Elapsed: 0:47:49.\n  Average Loss: 0.08298294704015918\n  Batch 24,600  of  26,258.    Elapsed: 0:48:01.\n  Average Loss: 0.08289505724791377\n  Batch 24,700  of  26,258.    Elapsed: 0:48:12.\n  Average Loss: 0.08278516494088357\n  Batch 24,800  of  26,258.    Elapsed: 0:48:24.\n  Average Loss: 0.08269408029635557\n  Batch 24,900  of  26,258.    Elapsed: 0:48:36.\n  Average Loss: 0.08260221504307086\n  Batch 25,000  of  26,258.    Elapsed: 0:48:48.\n  Average Loss: 0.08250545281704515\n  Batch 25,100  of  26,258.    Elapsed: 0:48:59.\n  Average Loss: 0.08241200778369914\n  Batch 25,200  of  26,258.    Elapsed: 0:49:11.\n  Average Loss: 0.0823204740839419\n  Batch 25,300  of  26,258.    Elapsed: 0:49:23.\n  Average Loss: 0.08223984219903208\n  Batch 25,400  of  26,258.    Elapsed: 0:49:35.\n  Average Loss: 0.08215460339733437\n  Batch 25,500  of  26,258.    Elapsed: 0:49:46.\n  Average Loss: 0.08205486071328906\n  Batch 25,600  of  26,258.    Elapsed: 0:49:58.\n  Average Loss: 0.08197225735639221\n  Batch 25,700  of  26,258.    Elapsed: 0:50:10.\n  Average Loss: 0.0818828923816173\n  Batch 25,800  of  26,258.    Elapsed: 0:50:22.\n  Average Loss: 0.08179301100270461\n  Batch 25,900  of  26,258.    Elapsed: 0:50:33.\n  Average Loss: 0.08170200407447502\n  Batch 26,000  of  26,258.    Elapsed: 0:50:45.\n  Average Loss: 0.08161109652277082\n  Batch 26,100  of  26,258.    Elapsed: 0:50:57.\n  Average Loss: 0.0815081440095729\n  Batch 26,200  of  26,258.    Elapsed: 0:51:09.\n  Average Loss: 0.08142101621632074\n\n  Average training loss: 0.08\n  Training epoch took: 0:51:15\n\nRunning Validation...\n  Accuracy: 0.87\n  Validation Loss: 0.05\n  Validation took: 0:20:05\n\n======== Epoch 2 / 2 ========\nTraining...\n  Batch   100  of  26,258.    Elapsed: 0:00:12.\n  Average Loss: 0.05443524067290127\n  Batch   200  of  26,258.    Elapsed: 0:00:23.\n  Average Loss: 0.05541489065159112\n  Batch   300  of  26,258.    Elapsed: 0:00:35.\n  Average Loss: 0.054594950759783385\n  Batch   400  of  26,258.    Elapsed: 0:00:47.\n  Average Loss: 0.055682248959783465\n  Batch   500  of  26,258.    Elapsed: 0:00:59.\n  Average Loss: 0.05542480667307973\n  Batch   600  of  26,258.    Elapsed: 0:01:10.\n  Average Loss: 0.05511187677582105\n  Batch   700  of  26,258.    Elapsed: 0:01:22.\n  Average Loss: 0.055538853092917374\n  Batch   800  of  26,258.    Elapsed: 0:01:34.\n  Average Loss: 0.05520395558211021\n  Batch   900  of  26,258.    Elapsed: 0:01:45.\n  Average Loss: 0.055170240272871325\n  Batch 1,000  of  26,258.    Elapsed: 0:01:57.\n  Average Loss: 0.0549959624055773\n  Batch 1,100  of  26,258.    Elapsed: 0:02:09.\n  Average Loss: 0.055179330362853676\n  Batch 1,200  of  26,258.    Elapsed: 0:02:20.\n  Average Loss: 0.054812188988241056\n  Batch 1,300  of  26,258.    Elapsed: 0:02:32.\n  Average Loss: 0.054902700240222306\n  Batch 1,400  of  26,258.    Elapsed: 0:02:44.\n  Average Loss: 0.05466800577073757\n  Batch 1,500  of  26,258.    Elapsed: 0:02:55.\n  Average Loss: 0.054708227641259634\n  Batch 1,600  of  26,258.    Elapsed: 0:03:07.\n  Average Loss: 0.054816165711381475\n  Batch 1,700  of  26,258.    Elapsed: 0:03:19.\n  Average Loss: 0.054672006320427444\n  Batch 1,800  of  26,258.    Elapsed: 0:03:31.\n  Average Loss: 0.05457783122805671\n  Batch 1,900  of  26,258.    Elapsed: 0:03:42.\n  Average Loss: 0.05470776697179597\n  Batch 2,000  of  26,258.    Elapsed: 0:03:54.\n  Average Loss: 0.05464369060983881\n  Batch 2,100  of  26,258.    Elapsed: 0:04:06.\n  Average Loss: 0.05454503311509533\n  Batch 2,200  of  26,258.    Elapsed: 0:04:17.\n  Average Loss: 0.05440755047158084\n  Batch 2,300  of  26,258.    Elapsed: 0:04:29.\n  Average Loss: 0.05445841398697508\n  Batch 2,400  of  26,258.    Elapsed: 0:04:41.\n  Average Loss: 0.05448066267805795\n  Batch 2,500  of  26,258.    Elapsed: 0:04:52.\n  Average Loss: 0.05452478052154183\n  Batch 2,600  of  26,258.    Elapsed: 0:05:04.\n  Average Loss: 0.05449007475354637\n  Batch 2,700  of  26,258.    Elapsed: 0:05:16.\n  Average Loss: 0.05451759747919385\n  Batch 2,800  of  26,258.    Elapsed: 0:05:27.\n  Average Loss: 0.0544693518526453\n  Batch 2,900  of  26,258.    Elapsed: 0:05:39.\n  Average Loss: 0.05440610593281173\n  Batch 3,000  of  26,258.    Elapsed: 0:05:51.\n  Average Loss: 0.054352589818648996\n  Batch 3,100  of  26,258.    Elapsed: 0:06:03.\n  Average Loss: 0.05423834365972829\n  Batch 3,200  of  26,258.    Elapsed: 0:06:14.\n  Average Loss: 0.05415994355920702\n  Batch 3,300  of  26,258.    Elapsed: 0:06:26.\n  Average Loss: 0.05421487341888926\n  Batch 3,400  of  26,258.    Elapsed: 0:06:38.\n  Average Loss: 0.05421680323341314\n  Batch 3,500  of  26,258.    Elapsed: 0:06:49.\n  Average Loss: 0.054175082314759496\n  Batch 3,600  of  26,258.    Elapsed: 0:07:01.\n  Average Loss: 0.05408697883313936\n  Batch 3,700  of  26,258.    Elapsed: 0:07:13.\n  Average Loss: 0.05400080260946541\n  Batch 3,800  of  26,258.    Elapsed: 0:07:24.\n  Average Loss: 0.05391154331841359\n  Batch 3,900  of  26,258.    Elapsed: 0:07:36.\n  Average Loss: 0.05392585430676356\n  Batch 4,000  of  26,258.    Elapsed: 0:07:48.\n  Average Loss: 0.053856152273947376\n  Batch 4,100  of  26,258.    Elapsed: 0:07:59.\n  Average Loss: 0.05381274736440945\n  Batch 4,200  of  26,258.    Elapsed: 0:08:11.\n  Average Loss: 0.05373170048286695\n  Batch 4,300  of  26,258.    Elapsed: 0:08:23.\n  Average Loss: 0.05375504046267029\n  Batch 4,400  of  26,258.    Elapsed: 0:08:35.\n  Average Loss: 0.05364598400742662\n  Batch 4,500  of  26,258.    Elapsed: 0:08:46.\n  Average Loss: 0.05358153088163171\n  Batch 4,600  of  26,258.    Elapsed: 0:08:58.\n  Average Loss: 0.05349946122434314\n  Batch 4,700  of  26,258.    Elapsed: 0:09:10.\n  Average Loss: 0.05347033174728301\n  Batch 4,800  of  26,258.    Elapsed: 0:09:21.\n  Average Loss: 0.05342960048680349\n  Batch 4,900  of  26,258.    Elapsed: 0:09:33.\n  Average Loss: 0.05345461679584518\n  Batch 5,000  of  26,258.    Elapsed: 0:09:45.\n  Average Loss: 0.05337923467978835\n  Batch 5,100  of  26,258.    Elapsed: 0:09:56.\n  Average Loss: 0.05334047899325835\n  Batch 5,200  of  26,258.    Elapsed: 0:10:08.\n  Average Loss: 0.053230086698447567\n  Batch 5,300  of  26,258.    Elapsed: 0:10:20.\n  Average Loss: 0.0532039867909098\n  Batch 5,400  of  26,258.    Elapsed: 0:10:31.\n  Average Loss: 0.05318794843482061\n  Batch 5,500  of  26,258.    Elapsed: 0:10:43.\n  Average Loss: 0.05313155340792781\n  Batch 5,600  of  26,258.    Elapsed: 0:10:55.\n  Average Loss: 0.053085824640667335\n  Batch 5,700  of  26,258.    Elapsed: 0:11:06.\n  Average Loss: 0.05301946937253601\n  Batch 5,800  of  26,258.    Elapsed: 0:11:18.\n  Average Loss: 0.052944473541534406\n  Batch 5,900  of  26,258.    Elapsed: 0:11:30.\n  Average Loss: 0.05288495973810175\n  Batch 6,000  of  26,258.    Elapsed: 0:11:41.\n  Average Loss: 0.05284476944198832\n  Batch 6,100  of  26,258.    Elapsed: 0:11:53.\n  Average Loss: 0.05283548795251695\n  Batch 6,200  of  26,258.    Elapsed: 0:12:05.\n  Average Loss: 0.052826619172588955\n  Batch 6,300  of  26,258.    Elapsed: 0:12:17.\n  Average Loss: 0.05275565674025861\n  Batch 6,400  of  26,258.    Elapsed: 0:12:28.\n  Average Loss: 0.05271998994023306\n  Batch 6,500  of  26,258.    Elapsed: 0:12:40.\n  Average Loss: 0.052689491109779246\n  Batch 6,600  of  26,258.    Elapsed: 0:12:52.\n  Average Loss: 0.05265105599297606\n  Batch 6,700  of  26,258.    Elapsed: 0:13:03.\n  Average Loss: 0.05255526105122669\n  Batch 6,800  of  26,258.    Elapsed: 0:13:15.\n  Average Loss: 0.05252299046212369\n  Batch 6,900  of  26,258.    Elapsed: 0:13:27.\n  Average Loss: 0.05246583603389993\n  Batch 7,000  of  26,258.    Elapsed: 0:13:38.\n  Average Loss: 0.052463968451667044\n  Batch 7,100  of  26,258.    Elapsed: 0:13:50.\n  Average Loss: 0.052412286014099356\n  Batch 7,200  of  26,258.    Elapsed: 0:14:02.\n  Average Loss: 0.0523581041307706\n  Batch 7,300  of  26,258.    Elapsed: 0:14:13.\n  Average Loss: 0.05228788213995732\n  Batch 7,400  of  26,258.    Elapsed: 0:14:25.\n  Average Loss: 0.05226276361660377\n  Batch 7,500  of  26,258.    Elapsed: 0:14:37.\n  Average Loss: 0.05220427903160453\n  Batch 7,600  of  26,258.    Elapsed: 0:14:48.\n  Average Loss: 0.0521710342711671\n  Batch 7,700  of  26,258.    Elapsed: 0:15:00.\n  Average Loss: 0.05209501360373741\n  Batch 7,800  of  26,258.    Elapsed: 0:15:12.\n  Average Loss: 0.052066314856474026\n  Batch 7,900  of  26,258.    Elapsed: 0:15:24.\n  Average Loss: 0.05201792647378354\n  Batch 8,000  of  26,258.    Elapsed: 0:15:35.\n  Average Loss: 0.051992009301204234\n  Batch 8,100  of  26,258.    Elapsed: 0:15:47.\n  Average Loss: 0.05196175115677402\n  Batch 8,200  of  26,258.    Elapsed: 0:15:59.\n  Average Loss: 0.05195159370409007\n  Batch 8,300  of  26,258.    Elapsed: 0:16:10.\n  Average Loss: 0.051879718959982314\n  Batch 8,400  of  26,258.    Elapsed: 0:16:22.\n  Average Loss: 0.05186704479945114\n  Batch 8,500  of  26,258.    Elapsed: 0:16:34.\n  Average Loss: 0.05182576427685426\n  Batch 8,600  of  26,258.    Elapsed: 0:16:45.\n  Average Loss: 0.051764583113430025\n  Batch 8,700  of  26,258.    Elapsed: 0:16:57.\n  Average Loss: 0.05175068251520995\n  Batch 8,800  of  26,258.    Elapsed: 0:17:09.\n  Average Loss: 0.05170238092012534\n  Batch 8,900  of  26,258.    Elapsed: 0:17:20.\n  Average Loss: 0.05168166150767984\n  Batch 9,000  of  26,258.    Elapsed: 0:17:32.\n  Average Loss: 0.05158801817376581\n  Batch 9,100  of  26,258.    Elapsed: 0:17:44.\n  Average Loss: 0.05154026944732682\n  Batch 9,200  of  26,258.    Elapsed: 0:17:55.\n  Average Loss: 0.05146610773801196\n  Batch 9,300  of  26,258.    Elapsed: 0:18:07.\n  Average Loss: 0.05143909831085713\n  Batch 9,400  of  26,258.    Elapsed: 0:18:19.\n  Average Loss: 0.051445565781447086\n  Batch 9,500  of  26,258.    Elapsed: 0:18:31.\n  Average Loss: 0.051368484456827374\n  Batch 9,600  of  26,258.    Elapsed: 0:18:42.\n  Average Loss: 0.051320723391885016\n  Batch 9,700  of  26,258.    Elapsed: 0:18:54.\n  Average Loss: 0.051290986748441056\n  Batch 9,800  of  26,258.    Elapsed: 0:19:06.\n  Average Loss: 0.05128599973238662\n  Batch 9,900  of  26,258.    Elapsed: 0:19:17.\n  Average Loss: 0.05122175221354936\n  Batch 10,000  of  26,258.    Elapsed: 0:19:29.\n  Average Loss: 0.05115348102985881\n  Batch 10,100  of  26,258.    Elapsed: 0:19:41.\n  Average Loss: 0.0510966206869885\n  Batch 10,200  of  26,258.    Elapsed: 0:19:52.\n  Average Loss: 0.05104639307883404\n  Batch 10,300  of  26,258.    Elapsed: 0:20:04.\n  Average Loss: 0.050997197358344584\n  Batch 10,400  of  26,258.    Elapsed: 0:20:16.\n  Average Loss: 0.050941701435009375\n  Batch 10,500  of  26,258.    Elapsed: 0:20:27.\n  Average Loss: 0.05089195384400054\n  Batch 10,600  of  26,258.    Elapsed: 0:20:39.\n  Average Loss: 0.05085069225165325\n  Batch 10,700  of  26,258.    Elapsed: 0:20:51.\n  Average Loss: 0.05080329809925371\n  Batch 10,800  of  26,258.    Elapsed: 0:21:03.\n  Average Loss: 0.05078811690896853\n  Batch 10,900  of  26,258.    Elapsed: 0:21:14.\n  Average Loss: 0.05072096966083136\n  Batch 11,000  of  26,258.    Elapsed: 0:21:26.\n  Average Loss: 0.05064830673900856\n  Batch 11,100  of  26,258.    Elapsed: 0:21:38.\n  Average Loss: 0.05060091015412933\n  Batch 11,200  of  26,258.    Elapsed: 0:21:49.\n  Average Loss: 0.05057250407773868\n  Batch 11,300  of  26,258.    Elapsed: 0:22:01.\n  Average Loss: 0.050516024512214076\n  Batch 11,400  of  26,258.    Elapsed: 0:22:13.\n  Average Loss: 0.05049171621461905\n  Batch 11,500  of  26,258.    Elapsed: 0:22:24.\n  Average Loss: 0.050462838472393544\n  Batch 11,600  of  26,258.    Elapsed: 0:22:36.\n  Average Loss: 0.050444906420026234\n  Batch 11,700  of  26,258.    Elapsed: 0:22:48.\n  Average Loss: 0.05037144541951358\n  Batch 11,800  of  26,258.    Elapsed: 0:23:00.\n  Average Loss: 0.050305693634446334\n  Batch 11,900  of  26,258.    Elapsed: 0:23:11.\n  Average Loss: 0.05026572774614387\n  Batch 12,000  of  26,258.    Elapsed: 0:23:23.\n  Average Loss: 0.05022780438764797\n  Batch 12,100  of  26,258.    Elapsed: 0:23:35.\n  Average Loss: 0.05019594978498803\n  Batch 12,200  of  26,258.    Elapsed: 0:23:46.\n  Average Loss: 0.05015177200857519\n  Batch 12,300  of  26,258.    Elapsed: 0:23:58.\n  Average Loss: 0.0501294150856528\n  Batch 12,400  of  26,258.    Elapsed: 0:24:10.\n  Average Loss: 0.05010576880720985\n  Batch 12,500  of  26,258.    Elapsed: 0:24:21.\n  Average Loss: 0.05008438835218549\n  Batch 12,600  of  26,258.    Elapsed: 0:24:33.\n  Average Loss: 0.05002483247402346\n  Batch 12,700  of  26,258.    Elapsed: 0:24:45.\n  Average Loss: 0.04998762276045626\n  Batch 12,800  of  26,258.    Elapsed: 0:24:56.\n  Average Loss: 0.049958292964365686\n  Batch 12,900  of  26,258.    Elapsed: 0:25:08.\n  Average Loss: 0.049937078295593226\n  Batch 13,000  of  26,258.    Elapsed: 0:25:20.\n  Average Loss: 0.04989256539339056\n  Batch 13,100  of  26,258.    Elapsed: 0:25:31.\n  Average Loss: 0.04983436577270902\n  Batch 13,200  of  26,258.    Elapsed: 0:25:43.\n  Average Loss: 0.04977656667014923\n  Batch 13,300  of  26,258.    Elapsed: 0:25:55.\n  Average Loss: 0.04973390824693654\n  Batch 13,400  of  26,258.    Elapsed: 0:26:07.\n  Average Loss: 0.049711604268783564\n  Batch 13,500  of  26,258.    Elapsed: 0:26:18.\n  Average Loss: 0.04965012723590351\n  Batch 13,600  of  26,258.    Elapsed: 0:26:30.\n  Average Loss: 0.04961149546985432\n  Batch 13,700  of  26,258.    Elapsed: 0:26:42.\n  Average Loss: 0.04957773599634287\n  Batch 13,800  of  26,258.    Elapsed: 0:26:53.\n  Average Loss: 0.049517325514934256\n  Batch 13,900  of  26,258.    Elapsed: 0:27:05.\n  Average Loss: 0.049492000702855664\n  Batch 14,000  of  26,258.    Elapsed: 0:27:17.\n  Average Loss: 0.04945415040647744\n  Batch 14,100  of  26,258.    Elapsed: 0:27:28.\n  Average Loss: 0.04941097605270036\n  Batch 14,200  of  26,258.    Elapsed: 0:27:40.\n  Average Loss: 0.04937001700208089\n  Batch 14,300  of  26,258.    Elapsed: 0:27:52.\n  Average Loss: 0.04931504254174009\n  Batch 14,400  of  26,258.    Elapsed: 0:28:03.\n  Average Loss: 0.04926727795252292\n  Batch 14,500  of  26,258.    Elapsed: 0:28:15.\n  Average Loss: 0.049241634921106545\n  Batch 14,600  of  26,258.    Elapsed: 0:28:27.\n  Average Loss: 0.0492271380293923\n  Batch 14,700  of  26,258.    Elapsed: 0:28:38.\n  Average Loss: 0.049202231852192004\n  Batch 14,800  of  26,258.    Elapsed: 0:28:50.\n  Average Loss: 0.04916113683409561\n  Batch 14,900  of  26,258.    Elapsed: 0:29:02.\n  Average Loss: 0.04912224142112138\n  Batch 15,000  of  26,258.    Elapsed: 0:29:14.\n  Average Loss: 0.049073778313212094\n  Batch 15,100  of  26,258.    Elapsed: 0:29:25.\n  Average Loss: 0.04903609867559265\n  Batch 15,200  of  26,258.    Elapsed: 0:29:37.\n  Average Loss: 0.0489856651205398\n  Batch 15,300  of  26,258.    Elapsed: 0:29:49.\n  Average Loss: 0.04894373147626669\n  Batch 15,400  of  26,258.    Elapsed: 0:30:00.\n  Average Loss: 0.04891107811108715\n  Batch 15,500  of  26,258.    Elapsed: 0:30:12.\n  Average Loss: 0.048878844913095236\n  Batch 15,600  of  26,258.    Elapsed: 0:30:24.\n  Average Loss: 0.048851150166517934\n  Batch 15,700  of  26,258.    Elapsed: 0:30:35.\n  Average Loss: 0.04879618115841773\n  Batch 15,800  of  26,258.    Elapsed: 0:30:47.\n  Average Loss: 0.048765625502984924\n  Batch 15,900  of  26,258.    Elapsed: 0:30:59.\n  Average Loss: 0.048732883653650354\n  Batch 16,000  of  26,258.    Elapsed: 0:31:11.\n  Average Loss: 0.048671742196049306\n  Batch 16,100  of  26,258.    Elapsed: 0:31:22.\n  Average Loss: 0.04862836441918861\n  Batch 16,200  of  26,258.    Elapsed: 0:31:34.\n  Average Loss: 0.048601101883600846\n  Batch 16,300  of  26,258.    Elapsed: 0:31:46.\n  Average Loss: 0.048575013809654566\n  Batch 16,400  of  26,258.    Elapsed: 0:31:57.\n  Average Loss: 0.048550088012153735\n  Batch 16,500  of  26,258.    Elapsed: 0:32:09.\n  Average Loss: 0.048520188815524855\n  Batch 16,600  of  26,258.    Elapsed: 0:32:21.\n  Average Loss: 0.04848498444546979\n  Batch 16,700  of  26,258.    Elapsed: 0:32:32.\n  Average Loss: 0.04844822400896447\n  Batch 16,800  of  26,258.    Elapsed: 0:32:44.\n  Average Loss: 0.048398881002905826\n  Batch 16,900  of  26,258.    Elapsed: 0:32:56.\n  Average Loss: 0.048361065330744316\n  Batch 17,000  of  26,258.    Elapsed: 0:33:07.\n  Average Loss: 0.04831636167460066\n  Batch 17,100  of  26,258.    Elapsed: 0:33:19.\n  Average Loss: 0.04827959235011442\n  Batch 17,200  of  26,258.    Elapsed: 0:33:31.\n  Average Loss: 0.048239034868040404\n  Batch 17,300  of  26,258.    Elapsed: 0:33:42.\n  Average Loss: 0.04819907148158705\n  Batch 17,400  of  26,258.    Elapsed: 0:33:54.\n  Average Loss: 0.04817219237251014\n  Batch 17,500  of  26,258.    Elapsed: 0:34:06.\n  Average Loss: 0.048131148613297516\n  Batch 17,600  of  26,258.    Elapsed: 0:34:18.\n  Average Loss: 0.04809290744041474\n  Batch 17,700  of  26,258.    Elapsed: 0:34:29.\n  Average Loss: 0.04806104431386595\n  Batch 17,800  of  26,258.    Elapsed: 0:34:41.\n  Average Loss: 0.04802387120275434\n  Batch 17,900  of  26,258.    Elapsed: 0:34:53.\n  Average Loss: 0.047985037258415165\n  Batch 18,000  of  26,258.    Elapsed: 0:35:04.\n  Average Loss: 0.047958893255641066\n  Batch 18,100  of  26,258.    Elapsed: 0:35:16.\n  Average Loss: 0.047916654936367264\n  Batch 18,200  of  26,258.    Elapsed: 0:35:28.\n  Average Loss: 0.04788674667027298\n  Batch 18,300  of  26,258.    Elapsed: 0:35:40.\n  Average Loss: 0.04786278113707534\n  Batch 18,400  of  26,258.    Elapsed: 0:35:51.\n  Average Loss: 0.047817637843975754\n  Batch 18,500  of  26,258.    Elapsed: 0:36:03.\n  Average Loss: 0.04777533107822308\n  Batch 18,600  of  26,258.    Elapsed: 0:36:15.\n  Average Loss: 0.047744712485275884\n  Batch 18,700  of  26,258.    Elapsed: 0:36:27.\n  Average Loss: 0.047708205768992915\n  Batch 18,800  of  26,258.    Elapsed: 0:36:38.\n  Average Loss: 0.04765795440333796\n  Batch 18,900  of  26,258.    Elapsed: 0:36:50.\n  Average Loss: 0.04763207948830708\n  Batch 19,000  of  26,258.    Elapsed: 0:37:02.\n  Average Loss: 0.04760930138689123\n  Batch 19,100  of  26,258.    Elapsed: 0:37:13.\n  Average Loss: 0.0475703339284793\n  Batch 19,200  of  26,258.    Elapsed: 0:37:25.\n  Average Loss: 0.04752789486194767\n  Batch 19,300  of  26,258.    Elapsed: 0:37:37.\n  Average Loss: 0.04747928862443068\n  Batch 19,400  of  26,258.    Elapsed: 0:37:49.\n  Average Loss: 0.04744559611196236\n  Batch 19,500  of  26,258.    Elapsed: 0:38:00.\n  Average Loss: 0.047415259627243266\n  Batch 19,600  of  26,258.    Elapsed: 0:38:12.\n  Average Loss: 0.047372625376886635\n  Batch 19,700  of  26,258.    Elapsed: 0:38:24.\n  Average Loss: 0.04735543291992153\n  Batch 19,800  of  26,258.    Elapsed: 0:38:36.\n  Average Loss: 0.04731919094978954\n  Batch 19,900  of  26,258.    Elapsed: 0:38:47.\n  Average Loss: 0.04727624473613617\n  Batch 20,000  of  26,258.    Elapsed: 0:38:59.\n  Average Loss: 0.047244009746448136\n  Batch 20,100  of  26,258.    Elapsed: 0:39:11.\n  Average Loss: 0.04721463239276828\n  Batch 20,200  of  26,258.    Elapsed: 0:39:23.\n  Average Loss: 0.04717911427354182\n  Batch 20,300  of  26,258.    Elapsed: 0:39:35.\n  Average Loss: 0.04715228834288112\n  Batch 20,400  of  26,258.    Elapsed: 0:39:46.\n  Average Loss: 0.047121432981129696\n  Batch 20,500  of  26,258.    Elapsed: 0:39:58.\n  Average Loss: 0.04708869513476313\n  Batch 20,600  of  26,258.    Elapsed: 0:40:10.\n  Average Loss: 0.04705147616748327\n  Batch 20,700  of  26,258.    Elapsed: 0:40:21.\n  Average Loss: 0.04701115744181679\n  Batch 20,800  of  26,258.    Elapsed: 0:40:33.\n  Average Loss: 0.04697748848870665\n  Batch 20,900  of  26,258.    Elapsed: 0:40:45.\n  Average Loss: 0.04692664051807205\n  Batch 21,000  of  26,258.    Elapsed: 0:40:57.\n  Average Loss: 0.04690376101647104\n  Batch 21,100  of  26,258.    Elapsed: 0:41:08.\n  Average Loss: 0.04687615128348789\n  Batch 21,200  of  26,258.    Elapsed: 0:41:20.\n  Average Loss: 0.04684307588945266\n  Batch 21,300  of  26,258.    Elapsed: 0:41:32.\n  Average Loss: 0.04681646820874841\n  Batch 21,400  of  26,258.    Elapsed: 0:41:43.\n  Average Loss: 0.046784181302352415\n  Batch 21,500  of  26,258.    Elapsed: 0:41:55.\n  Average Loss: 0.04675259130214189\n  Batch 21,600  of  26,258.    Elapsed: 0:42:07.\n  Average Loss: 0.046723493662990494\n  Batch 21,700  of  26,258.    Elapsed: 0:42:19.\n  Average Loss: 0.04669588124164925\n  Batch 21,800  of  26,258.    Elapsed: 0:42:30.\n  Average Loss: 0.046670928747359015\n  Batch 21,900  of  26,258.    Elapsed: 0:42:42.\n  Average Loss: 0.04663033190851002\n  Batch 22,000  of  26,258.    Elapsed: 0:42:54.\n  Average Loss: 0.046606356788849966\n  Batch 22,100  of  26,258.    Elapsed: 0:43:05.\n  Average Loss: 0.04657109887535132\n  Batch 22,200  of  26,258.    Elapsed: 0:43:17.\n  Average Loss: 0.04653710849916663\n  Batch 22,300  of  26,258.    Elapsed: 0:43:29.\n  Average Loss: 0.04650851286778586\n  Batch 22,400  of  26,258.    Elapsed: 0:43:41.\n  Average Loss: 0.04647288527433245\n  Batch 22,500  of  26,258.    Elapsed: 0:43:52.\n  Average Loss: 0.04645305542227709\n  Batch 22,600  of  26,258.    Elapsed: 0:44:04.\n  Average Loss: 0.04640870086348047\n  Batch 22,700  of  26,258.    Elapsed: 0:44:16.\n  Average Loss: 0.04639300822704981\n  Batch 22,800  of  26,258.    Elapsed: 0:44:27.\n  Average Loss: 0.046365891837028855\n  Batch 22,900  of  26,258.    Elapsed: 0:44:39.\n  Average Loss: 0.04633784199269757\n  Batch 23,000  of  26,258.    Elapsed: 0:44:51.\n  Average Loss: 0.04630550916839148\n  Batch 23,100  of  26,258.    Elapsed: 0:45:03.\n  Average Loss: 0.04626695324225327\n  Batch 23,200  of  26,258.    Elapsed: 0:45:14.\n  Average Loss: 0.04622935228790397\n  Batch 23,300  of  26,258.    Elapsed: 0:45:26.\n  Average Loss: 0.04620082261346584\n  Batch 23,400  of  26,258.    Elapsed: 0:45:38.\n  Average Loss: 0.046172616878030905\n  Batch 23,500  of  26,258.    Elapsed: 0:45:49.\n  Average Loss: 0.04613622745142338\n  Batch 23,600  of  26,258.    Elapsed: 0:46:01.\n  Average Loss: 0.04610318619805246\n  Batch 23,700  of  26,258.    Elapsed: 0:46:13.\n  Average Loss: 0.046088253908067596\n  Batch 23,800  of  26,258.    Elapsed: 0:46:24.\n  Average Loss: 0.04605952269978774\n  Batch 23,900  of  26,258.    Elapsed: 0:46:36.\n  Average Loss: 0.04603314248020819\n  Batch 24,000  of  26,258.    Elapsed: 0:46:48.\n  Average Loss: 0.045996648757272246\n  Batch 24,100  of  26,258.    Elapsed: 0:47:00.\n  Average Loss: 0.0459609119436236\n  Batch 24,200  of  26,258.    Elapsed: 0:47:11.\n  Average Loss: 0.045930719927380494\n  Batch 24,300  of  26,258.    Elapsed: 0:47:23.\n  Average Loss: 0.045898182769746755\n  Batch 24,400  of  26,258.    Elapsed: 0:47:35.\n  Average Loss: 0.045862689605669776\n  Batch 24,500  of  26,258.    Elapsed: 0:47:47.\n  Average Loss: 0.04584039995275742\n  Batch 24,600  of  26,258.    Elapsed: 0:47:58.\n  Average Loss: 0.04581577104444396\n  Batch 24,700  of  26,258.    Elapsed: 0:48:10.\n  Average Loss: 0.045787992508637035\n  Batch 24,800  of  26,258.    Elapsed: 0:48:22.\n  Average Loss: 0.0457557207756796\n  Batch 24,900  of  26,258.    Elapsed: 0:48:33.\n  Average Loss: 0.04571684987351567\n  Batch 25,000  of  26,258.    Elapsed: 0:48:45.\n  Average Loss: 0.04568280670080334\n  Batch 25,100  of  26,258.    Elapsed: 0:48:57.\n  Average Loss: 0.04565694113644174\n  Batch 25,200  of  26,258.    Elapsed: 0:49:08.\n  Average Loss: 0.045626935341750226\n  Batch 25,300  of  26,258.    Elapsed: 0:49:20.\n  Average Loss: 0.04559950239014013\n  Batch 25,400  of  26,258.    Elapsed: 0:49:32.\n  Average Loss: 0.04556446788766546\n  Batch 25,500  of  26,258.    Elapsed: 0:49:44.\n  Average Loss: 0.04553607256860272\n  Batch 25,600  of  26,258.    Elapsed: 0:49:55.\n  Average Loss: 0.04551277994149132\n  Batch 25,700  of  26,258.    Elapsed: 0:50:07.\n  Average Loss: 0.045478538607268915\n  Batch 25,800  of  26,258.    Elapsed: 0:50:19.\n  Average Loss: 0.0454593413898743\n  Batch 25,900  of  26,258.    Elapsed: 0:50:30.\n  Average Loss: 0.04542009020946977\n  Batch 26,000  of  26,258.    Elapsed: 0:50:42.\n  Average Loss: 0.04538613489202152\n  Batch 26,100  of  26,258.    Elapsed: 0:50:54.\n  Average Loss: 0.04535883999590216\n  Batch 26,200  of  26,258.    Elapsed: 0:51:05.\n  Average Loss: 0.04534270685845062\n\n  Average training loss: 0.05\n  Training epoch took: 0:51:12\n\nRunning Validation...\n  Accuracy: 0.92\n  Validation Loss: 0.03\n  Validation took: 0:20:04\n\nTraining complete!\nTotal training took 2:22:37 (h:mm:ss)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Итоговая точность на валидации:"
      ],
      "metadata": {
        "id": "4SHBIZ1myyS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_stats[-1]['Valid. Accur.']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-19T17:28:25.542280Z",
          "iopub.execute_input": "2023-11-19T17:28:25.542645Z",
          "iopub.status.idle": "2023-11-19T17:28:25.548731Z",
          "shell.execute_reply.started": "2023-11-19T17:28:25.542613Z",
          "shell.execute_reply": "2023-11-19T17:28:25.547687Z"
        },
        "trusted": true,
        "id": "fJcZjfHvyyS1",
        "outputId": "d9e79cc4-022b-4683-a3c1-94cebf82b542"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 130,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.9155790525499483"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Точность на трейне:"
      ],
      "metadata": {
        "id": "E4Mp0w2CyyS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-19T17:43:06.214158Z",
          "iopub.execute_input": "2023-11-19T17:43:06.214529Z",
          "iopub.status.idle": "2023-11-19T17:43:06.218981Z",
          "shell.execute_reply.started": "2023-11-19T17:43:06.214497Z",
          "shell.execute_reply": "2023-11-19T17:43:06.217900Z"
        },
        "trusted": true,
        "id": "r22WlaS-yyS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "total_train_accuracy = 0\n",
        "\n",
        "for batch in tqdm(train_dataloader):\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(b_input_ids,\n",
        "                       token_type_ids=None,\n",
        "                       attention_mask=b_input_mask,\n",
        "                       labels=b_labels)\n",
        "\n",
        "    logits = output.logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    total_train_accuracy += accuracy(logits, label_ids)\n",
        "\n",
        "avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n",
        "print(f\"Train accuracy: {avg_train_accuracy}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-19T17:43:37.707809Z",
          "iopub.execute_input": "2023-11-19T17:43:37.708193Z",
          "iopub.status.idle": "2023-11-19T18:04:00.719435Z",
          "shell.execute_reply.started": "2023-11-19T17:43:37.708161Z",
          "shell.execute_reply": "2023-11-19T18:04:00.716620Z"
        },
        "trusted": true,
        "id": "nTxeVuezyyS2",
        "outputId": "5696c987-ebfb-45f9-f43e-70024e8d7523",
        "colab": {
          "referenced_widgets": [
            "ed78576aae7149e09f19ec8ee7eb2391"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/26258 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed78576aae7149e09f19ec8ee7eb2391"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Train accuracy: 0.9252882454870897\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# state = {\n",
        "#     'state_dict': model.state_dict(),\n",
        "# }\n",
        "# torch.save(state, 'deberta_91_accuracy.pth')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-19T16:07:30.510185Z",
          "iopub.execute_input": "2023-11-19T16:07:30.510980Z",
          "iopub.status.idle": "2023-11-19T16:07:30.776705Z",
          "shell.execute_reply.started": "2023-11-19T16:07:30.510940Z",
          "shell.execute_reply": "2023-11-19T16:07:30.775810Z"
        },
        "trusted": true,
        "id": "CfIHlZi7yyS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(word):\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "            word,\n",
        "            add_special_tokens = True,\n",
        "            max_length = 64,\n",
        "            padding='max_length',\n",
        "            return_attention_mask = True,\n",
        "            return_tensors = 'pt',\n",
        "           )\n",
        "    input_ids = encoded_dict['input_ids'].to(device)\n",
        "    attention_mask = encoded_dict['attention_mask'].to(device)\n",
        "    output = model(input_ids,\n",
        "                token_type_ids=None,\n",
        "                attention_mask=attention_mask)\n",
        "    label = output.logits.detach().cpu().numpy().argmax(axis=2)[0]\n",
        "    input_ids = input_ids.cpu().numpy()[0]\n",
        "\n",
        "    letters = list(word)\n",
        "    stresses_count = 0\n",
        "    for i, stress in enumerate(label[1:1+len(word)]):\n",
        "        if stress:\n",
        "            letters.insert(i + stresses_count, '^')\n",
        "            stresses_count += 1\n",
        "    return ''.join(letters)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-19T17:05:32.124500Z",
          "iopub.execute_input": "2023-11-19T17:05:32.125253Z",
          "iopub.status.idle": "2023-11-19T17:05:32.132636Z",
          "shell.execute_reply.started": "2023-11-19T17:05:32.125216Z",
          "shell.execute_reply": "2023-11-19T17:05:32.131691Z"
        },
        "trusted": true,
        "id": "69Qkro9uyyS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_sentence(sentence):\n",
        "    words = sentence.split()\n",
        "    return ' '.join([test(word) for word in words])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-19T17:23:31.885278Z",
          "iopub.execute_input": "2023-11-19T17:23:31.885888Z",
          "iopub.status.idle": "2023-11-19T17:23:31.890515Z",
          "shell.execute_reply.started": "2023-11-19T17:23:31.885852Z",
          "shell.execute_reply": "2023-11-19T17:23:31.889530Z"
        },
        "trusted": true,
        "id": "eFEsThxZyyS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence('Трансформер обучен!')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-19T17:30:37.142603Z",
          "iopub.execute_input": "2023-11-19T17:30:37.143732Z",
          "iopub.status.idle": "2023-11-19T17:30:37.162200Z",
          "shell.execute_reply.started": "2023-11-19T17:30:37.143680Z",
          "shell.execute_reply": "2023-11-19T17:30:37.161324Z"
        },
        "trusted": true,
        "id": "FUIxWXybyyS2",
        "outputId": "fa6d32da-8e2b-4b5f-faa6-e87e3861935e"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 131,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'Трансф^ормер об^учен!'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence('Домашнее задание выполнено.')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-19T17:33:29.266374Z",
          "iopub.execute_input": "2023-11-19T17:33:29.266762Z",
          "iopub.status.idle": "2023-11-19T17:33:29.290938Z",
          "shell.execute_reply.started": "2023-11-19T17:33:29.266728Z",
          "shell.execute_reply": "2023-11-19T17:33:29.290019Z"
        },
        "trusted": true,
        "id": "whlEXbJ_yyS3",
        "outputId": "e1a0b4c2-8658-4cfb-a80e-95ad108adced"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 140,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'Дом^ашнее зад^ание в^ыполнено.'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence('Хочу на стажировочку во ВКонтакте')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-19T17:24:03.077883Z",
          "iopub.execute_input": "2023-11-19T17:24:03.078836Z",
          "iopub.status.idle": "2023-11-19T17:24:03.125223Z",
          "shell.execute_reply.started": "2023-11-19T17:24:03.078791Z",
          "shell.execute_reply": "2023-11-19T17:24:03.124261Z"
        },
        "trusted": true,
        "id": "qKBnHYZxyyS3",
        "outputId": "b596dee8-dc26-4b2b-b716-ed2dedcee0e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 126,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'Хоч^у н^а стажир^овочку в^о ВКонт^акте'"
          },
          "metadata": {}
        }
      ]
    }
  ]
}